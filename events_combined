# Add imports
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
from dateutil.parser import parse  # Flexible date parsing

# Set up Selenium WebDriver
def get_dynamic_html(url):
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    driver = webdriver.Chrome(options=chrome_options)

    try:
        driver.get(url)
        driver.implicitly_wait(10)
        html_content = driver.page_source
    except Exception as e:
        print(f"Error: {e}")
        html_content = ""
    finally:
        driver.quit()

    return html_content

# Extract events from MMJCCM
def extract_mmjccm_events(url):
    html_content = get_dynamic_html(url)
    soup = BeautifulSoup(html_content, "html.parser")
    
    event_data = []
    organizer = "Marlene Meyerson JCC Manhattan"
    
    for event in soup.find_all("div", class_="program-item"):
        event_title = event.find("h3").text.strip() if event.find("h3") else "No Title Provided"
        event_url = event.find("a")["href"] if event.find("a") else "URL Not Provided"
        event_date_time = event.find("div", class_="datetime").text.strip() if event.find("div", class_="datetime") else "No Date/Time Provided"
        event_location = "Marlene Meyerson JCC Manhattan"
        
        # Parse date and time
        try:
            event_date, event_time = event_date_time.split("|")
            event_date = event_date.strip()
            event_time = event_time.strip()
            weekday_name = datetime.strptime(event_date, "%B %d, %Y").strftime("%A")
        except Exception as e:
            print(f"Error parsing MMJCCM date: {e}")
            weekday_name, event_date, event_time = "Invalid Date", "Invalid Date", "Invalid Time"
        
        event_data.append({
            "Weekday": weekday_name,
            "Date": event_date,
            "Start Time": event_time,
            "Event Title": event_title,
            "Event Location": event_location,
            "Organizer": organizer,
            "Source": event_url
        })
    
    return event_data

# Extract events from Mustang Harry's
def extract_mustang_events(url):
    html_content = get_dynamic_html(url)
    soup = BeautifulSoup(html_content, "html.parser")
    
    event_data = []
    organizer = "Mustang Harry's"
    
    for event in soup.find_all("div", class_="event-block"):
        event_title = event.find("h3").text.strip() if event.find("h3") else "No Title Provided"
        event_url = event.find("a")["href"] if event.find("a") else "URL Not Provided"
        event_date_time = event.find("p", class_="event-date").text.strip() if event.find("p", class_="event-date") else "No Date/Time Provided"
        event_location = "Mustang Harry's, NYC"
        
        # Parse date and time
        try:
            event_date, event_time = event_date_time.split("|")
            event_date = event_date.strip()
            event_time = event_time.strip()
            weekday_name = datetime.strptime(event_date, "%B %d, %Y").strftime("%A")
        except Exception as e:
            print(f"Error parsing Mustang Harry's date: {e}")
            weekday_name, event_date, event_time = "Invalid Date", "Invalid Date", "Invalid Time"
        
        event_data.append({
            "Weekday": weekday_name,
            "Date": event_date,
            "Start Time": event_time,
            "Event Title": event_title,
            "Event Location": event_location,
            "Organizer": organizer,
            "Source": event_url
        })
    
    return event_data

# Extract events from 92NY
def extract_92ny_events(url):
    html_content = get_dynamic_html(url)
    soup = BeautifulSoup(html_content, "html.parser")
    
    event_data = []
    organizer = "92NY"
    
    for event in soup.find_all("div", class_="event-list-item"):
        event_title = event.find("h4").text.strip() if event.find("h4") else "No Title Provided"
        event_url = event.find("a")["href"] if event.find("a") else "URL Not Provided"
        event_date_time = event.find("div", class_="event-datetime").text.strip() if event.find("div", class_="event-datetime") else "No Date/Time Provided"
        event_location = "92NY, NYC"
        
        # Parse date and time
        try:
            event_date, event_time = event_date_time.split("|")
            event_date = event_date.strip()
            event_time = event_time.strip()
            weekday_name = datetime.strptime(event_date, "%B %d, %Y").strftime("%A")
        except Exception as e:
            print(f"Error parsing 92NY date: {e}")
            weekday_name, event_date, event_time = "Invalid Date", "Invalid Date", "Invalid Time"
        
        event_data.append({
            "Weekday": weekday_name,
            "Date": event_date,
            "Start Time": event_time,
            "Event Title": event_title,
            "Event Location": event_location,
            "Organizer": organizer,
            "Source": event_url
        })
    
    return event_data

# Extract events from Paley Center
def extract_paley_events(url):
    html_content = get_dynamic_html(url)
    soup = BeautifulSoup(html_content, "html.parser")
    
    event_data = []
    organizer = "Paley Center for Media"
    
    for event in soup.find_all("div", class_="event-item"):
        event_title = event.find("h3").text.strip() if event.find("h3") else "No Title Provided"
        event_url = event.find("a")["href"] if event.find("a") else "URL Not Provided"
        event_date_time = event.find("span", class_="date").text.strip() if event.find("span", class_="date") else "No Date/Time Provided"
        event_location = "Paley Center, NYC"
        
        # Parse date and time
        try:
            event_date, event_time = event_date_time.split("|")
            event_date = event_date.strip()
            event_time = event_time.strip()
            weekday_name = datetime.strptime(event_date, "%B %d, %Y").strftime("%A")
        except Exception as e:
            print(f"Error parsing Paley Center date: {e}")
            weekday_name, event_date, event_time = "Invalid Date", "Invalid Date", "Invalid Time"
        
        event_data.append({
            "Weekday": weekday_name,
            "Date": event_date,
            "Start Time": event_time,
            "Event Title": event_title,
            "Event Location": event_location,
            "Organizer": organizer,
            "Source": event_url
        })
    
    return event_data

# Extract Fleetfeet Hoboken events
def generate_fleetfeet_events():
    event_data = []
    organizer = "Fleetfeet Hoboken"
    event_location = "Fleetfeet Hoboken"
    event_title = "Fleetfeet Hoboken Group Run"
    event_source = "https://www.fleetfeet.com/s/hoboken/events/group-runs"
    event_time = "7:00pm"

    # Find the next Wednesday
    start_date = datetime.now()
    while start_date.weekday() != 2:  # 2 represents Wednesday
        start_date += timedelta(days=1)

    # Generate events for the next 6 months (approximately 26 weeks)
    for _ in range(26):
        weekday_name = start_date.strftime("%A")
        event_date = start_date.strftime("%Y-%m-%d")
        
        event_data.append({
            "Weekday": weekday_name,
            "Date": event_date,
            "Start Time": event_time,
            "Event Title": event_title,
            "Event Location": event_location,
            "Organizer": organizer,
            "Source": event_source
        })
        
        # Move to the next Wednesday
        start_date += timedelta(weeks=1)
    
    return event_data


# URLs of the event pages
fc_url = "https://www.friendshipcirclenyc.org/events"
uja_url = "https://www.ujafedny.org/get-involved/attend-an-event"
mmjccm_url = "https://mmjccm.org/programs?keywords=&start=&end=&category=1596,1676&sort-by=date"
mustang_url = "https://www.mustangharrys.com/events/"
ny92_url = "https://www.92ny.org/events"
paley_url = "https://www.paleycenter.org/events/"
fh_url = "https://www.fleetfeet.com/s/hoboken/events/group-runs"

# Extract events from all sources
fc_events = extract_fc_events(fc_url)
uja_events = extract_uja_events(uja_url)
mmjccm_events = extract_mmjccm_events(mmjccm_url)
mustang_events = extract_mustang_events(mustang_url)
ny92_events = extract_92ny_events(ny92_url)
paley_events = extract_paley_events(paley_url)
fh_events = generate_fleetfeet_events()

# Combine data into a single DataFrame
all_events = fc_events + uja_events + mmjccm_events + mustang_events + ny92_events + paley_events + fh_events
df = pd.DataFrame(all_events)

# Format and sort DataFrame
df['DateTime'] = pd.to_datetime(df['Date'] + " " + df['Start Time'], errors='coerce')
df = df.sort_values(by='DateTime').drop(columns=['DateTime'])

# Convert Source column to Excel-friendly hyperlinks
df['Source'] = df['Source'].apply(lambda url: f'=HYPERLINK("{url}", "Event Page")' if url != "URL Not Provided" else "URL Not Provided")

# Reorder columns so that 'Event Title' is at the end
df = df[['Weekday', 'Date', 'Start Time', 'Event Location', 'Organizer', 'Source', 'Event Title']]

# Save sorted data to a CSV file
output_file = r"C:\Users\JoshuaGoldberg\OneDrive - happify.com\Desktop\Event Calendar\events.csv"
df.to_csv(output_file, index=False)
print(f"Data saved to {output_file}")

# Add print for verification
print(df.head(20))
